
# AUTOMATICALLY GENERATED
# DO NOT EDIT THIS FILE DIRECTLY, USE /templates/conf/fluent.conf.erb

@include systemd.conf
@include kubernetes.conf

<match **>
  @type copy
  <store>
    @type elasticsearch
    @id out_es
    @log_level info
    include_tag_key true
    host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
    port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
    scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
    ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'false'}"
    user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
    password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
    reload_connections "#{ENV['FLUENT_ELASTICSEARCH_RELOAD_CONNECTIONS'] || 'true'}"
    logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
    logstash_format true
    type_name fluentd
    <buffer>
      flush_thread_count "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_THREAD_COUNT'] || '8'}"
      flush_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_FLUSH_INTERVAL'] || '5s'}"
      chunk_limit_size "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_CHUNK_LIMIT_SIZE'] || '2M'}"
      queue_limit_length "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_QUEUE_LIMIT_LENGTH'] || '32'}"
      retry_max_interval "#{ENV['FLUENT_ELASTICSEARCH_BUFFER_RETRY_MAX_INTERVAL'] || '30'}"
      retry_forever true
    </buffer>
  </store>
  <store>
    @type kinesis_streams
    @id out_kinesis_streams
    region "#{ENV['FLUENT_KINESIS_STREAMS_REGION'] || nil}"
    stream_name "#{ENV['FLUENT_KINESIS_STREAMS_STREAM_NAME']}"
    include_time_key "#{ENV['FLUENT_KINESIS_STREAMS_INCLUDE_TIME_KEY'] || false}"
    aws_key_id "#{ENV['FLUENT_KINESIS_STREAMS_AWS_KEY_ID']}"
    aws_sec_key "#{ENV['FLUENT_KINESIS_STREAMS_AWS_SEC_KEY']}"
    <buffer>
      flush_interval 1
      chunk_limit_size 1m
      flush_thread_interval 0.1
      flush_thread_burst_interval 0.01
      flush_thread_count 15
    </buffer>
  </store>
  <store>
    # docs: https://docs.fluentd.org/v0.12/articles/out_s3
    # note: this configuration relies on the nodes have an IAM instance profile with access to your S3 bucket
    @type s3
    @id out_s3
    @log_level info
    s3_bucket "#{ENV['FLUENT_S3_BUCKET_NAME']}"
    s3_region "#{ENV['FLUENT_S3_BUCKET_REGION']}"
    aws_key_id "#{ENV['FLUENT_S3_AWS_KEY_ID']}"
    aws_sec_key "#{ENV['FLUENT_S3_AWS_SEC_KEY']}"
    s3_object_key_format %{path}%Y/%m/%d/cluster-log-%{index}.%{file_extension}
    <inject>
      time_key time
      tag_key tag
      localtime false
    </inject>  
    <buffer>
      @type file
      path /var/log/fluentd-buffers/s3.buffer
      timekey 3600
      timekey_use_utc true
      chunk_limit_size 256m
    </buffer> 
  </store>
</match>
